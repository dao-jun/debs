# Storage Service

A distributed storage system built on AWS/Aliyun EBS with multi-attach support.

## Architecture

The storage service is designed to leverage cloud-native block storage (EBS) with multi-attach capabilities to build a distributed storage system without implementing our own replication mechanism.

### Key Components

1. **Metadata Module** (`pkg/metadata`)
    - Manages shard-to-volume and volume-to-node mappings
    - Interface-based design for external consensus systems (etcd, ZooKeeper, etc.)
    - Handles shard ID generation and metadata operations
    - Includes RetryWrapper for automatic retry with exponential backoff on transient failures

2. **VolumeManager** (`pkg/volume`)
    - Manages EBS volume operations on the local node
    - Handles mounting/unmounting of volumes
    - Supports multi-attach with primary/backup node roles
    - Volumes are mounted at `/{volumeID}`

3. **ShardManager** (`pkg/shard`)
    - Manages shards on the local node
    - Creates, opens, closes, and deletes shards
    - Each shard is stored in a directory: `/{volumeID}/shard-{shardId}`

4. **Shard** (`pkg/shard`)
    - Individual data segment stored using Pebble (RocksDB for Go)
    - Supports read-write and read-only modes
    - Size limit: configurable (e.g., 2GB per shard)

5. **Cloud Provider** (`pkg/cloud`)
    - Abstracted interface for multi-cloud support
    - AWS EBS implementation provided
    - Supports multi-attach enabled volumes

6. **gRPC Server** (`pkg/server`)
    - Implements the StorageService gRPC interface
    - Handles Put, Get, BatchPut, BatchGet operations
    - Manages shard lifecycle (NewShard, CloseShard, DeleteShard)

## Multi-Attach Architecture

The system uses EBS multi-attach to allow multiple nodes to access the same volume:

- **Primary Node**: Has read-write access to the volume
- **Backup Nodes**: Have read-only access and can be promoted to primary during failover

When a primary node fails, a backup node can:
1. Remount the volume as read-write
2. Update metadata to indicate it's now the primary
3. Continue serving requests

## Protocol Buffers

The service uses Protocol Buffers for client-server communication. See `proto/storage.proto` for the full API definition.

### Key Operations

- `Put`: Store a single entry with optional indexes
- `BatchPut`: Store multiple entries in one request
- `Get`: Retrieve a single entry
- `BatchGet`: Retrieve multiple entries (streaming response)
- `NewShard`: Create a new shard with a unique ID
- `CloseShard`: Mark a shard as read-only
- `DeleteShard`: Delete a shard and its data

## Building

```bash
# Install development tools
make install-tools

# Build the server
make build

# Or manually:
go mod tidy
make proto
go build -o bin/storage-server cmd/server/main.go
```

## Running

```bash
# Set environment variables
export NODE_ID=node-1
export INSTANCE_ID=i-1234567890abcdef0
export AWS_REGION=us-west-2

# Run the server
make run

# Or run directly:
./bin/storage-server
```

## Usage Example

See `examples/client/main.go` for a complete example of using the client API.

```bash
# Build and run the example client (requires server running)
cd examples/client
go run main.go
```

## Configuration

The server can be configured via:
- Configuration file (YAML)
- Environment variables
- Command-line flags

### Environment Variables

- `NODE_ID`: Unique identifier for this node
- `INSTANCE_ID`: Cloud instance ID (EC2 instance ID)
- `AWS_REGION`: AWS region for EBS operations

## Implementation Notes

### Shard ID Generation

Shard IDs are globally unique and monotonically increasing. They are generated by the metadata store using an external consensus system to ensure uniqueness across all nodes.

### Volume Selection

When creating a new shard, the system selects an available volume based on:
- Available capacity
- Current node's primary volumes
- Load balancing across volumes

### Failover

The system supports automatic failover when a primary node fails:
1. External monitoring detects node failure
2. A backup node is selected to become primary
3. Volume is remounted as read-write
4. Metadata is updated
5. Service continues with minimal downtime

### Data Consistency

- EBS provides block-level consistency
- Pebble provides LSM-tree based storage with write-ahead logging
- Multi-attach ensures data is accessible even during primary node failure
- Read-only mounts on backup nodes prevent data corruption

## Testing

```bash
# Run tests
make test

# Run tests with coverage
make test-coverage
```

See `pkg/shard/shard_test.go` for example tests.

## Future Enhancements

1. **Metadata Store Implementations**
    - etcd implementation
    - ZooKeeper implementation
    - Consul implementation

2. **Cloud Provider Support**
    - Aliyun EBS implementation
    - GCP Persistent Disk support

3. **Advanced Features**
    - Automatic volume provisioning
    - Shard migration between volumes
    - Automatic failover detection
    - Volume snapshot and backup
    - Metrics and monitoring integration

4. **Performance Optimizations**
    - Caching layer
    - Batch operation optimizations
    - Compression support
    - Index optimization

## Documentation

- [DEPLOYMENT.md](DEPLOYMENT.md) - Comprehensive deployment guide for AWS
- [config/example.yaml](config/example.yaml) - Example configuration file
- [examples/client/main.go](examples/client/main.go) - Client usage examples

## Contributing

Contributions are welcome! Please ensure:
1. Code follows Go best practices
2. Tests are included for new features
3. Documentation is updated
4. All tests pass: `make test`

## License

[Add your license here]